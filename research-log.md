Legend:
- ğŸ“œ: papers
- ğŸ“°: blog posts, project pages
- ğŸ“–: books
- ğŸŒ: broad/general resources
- ğŸ§ª: code, experiments
- ğŸ“º: videos

06/21/2024
- ğŸ“œ[Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data](https://arxiv.org/pdf/2406.14546)
    - inductive out-of-context reasoning (OOCR): infer value of latent information during (finetuning) training. High variance, but better than ICL. GPT-4 did better than GPT-3.5
- ğŸ“°[Attention Output SAEs Improve Circuit Analysis](https://www.alignmentforum.org/posts/EGvtgB7ctifzxZg6v/attention-output-saes-improve-circuit-analysis)
    - Trained SAEs on every layer of GPT-2 Small, did interp on IOI circuit, built [Circuit Explorer](https://d483a8995f67545839dbee56b8c864fca.clg07azjl.paperspacegradient.com/) tool for recursive DFA. Acknowledge SAEs are still unreliable

06/20/2024
- ğŸ“œ[Safety Cases: How to Justify the Safety of Advanced AI Systems](https://arxiv.org/pdf/2403.10462)
    - Proposes framework for decomposing complex AI systems & indentifying arguments for reaching acceptably low risk in ability, control, trustworthiness, & deference
- ğŸ“°[UK AI SI Inspect](https://ukgovernmentbeis.github.io/inspect_ai/)
    - OSS evals framework. Flexible, supports popular providers
- ğŸ§ª[Experimented with Inspect](./projects/inspect-experiment/)
    - submitted [bug fix](https://github.com/UKGovernmentBEIS/inspect_ai/pull/58)
- ğŸ“–[AI Safety Book (Hendrycks), Chapters 6.1-6.10](https://drive.google.com/file/d/1JN7-ZGx9KLqRJ94rOQVwRSa7FPZGl2OY/view)
    - covered machine ethics & a variety of approaches to align AI to human values, preferences, utility

06/19/2024
- ğŸ§ª[Experimented with using control vectors to steer Llama 3 8B](./projects/repengy/repengy.ipynb)
    - only succeeded with very simple controls (e.g. all-caps). Suspect this scales better with larger models

06/18/2024
- ğŸ“º[METR's Talk on Evaluations Research - Beth Barnes, Daniel Ziegler, Ted Suzman](https://www.youtube.com/watch?v=KO72xvYAP-w)
- ğŸ“–[Foundational Challenges in Assuring Alignment and Safety of Large Language Models](https://llm-safety-challenges.github.io/challenges_llms.pdf)
- ğŸ“œ[Pretraining Language Models with Human Preferences](https://arxiv.org/pdf/2302.08582)
- ğŸ“œ[KTO: Model Alignment as Prospect Theoretic Optimization](https://arxiv.org/pdf/2402.01306)
- ğŸ“œ[Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought](https://arxiv.org/pdf/2403.05518)
- ğŸ“œ[Sycophancy To Subterfuge: Investigating Reward Tampering In Language Models](https://arxiv.org/pdf/2406.10162)

06/17/2024
- ğŸ“œ[Supervising strong learners by amplifying weak experts](https://arxiv.org/pdf/1810.08575)
- ğŸ“œ[Scalable agent alignment via reward modeling: a research direction](https://arxiv.org/pdf/1811.07871)
- ğŸ“œ[AI safety via debate](https://arxiv.org/pdf/1805.00899)
- ğŸ“œ[Self-critiquing models for assisting human evaluators](https://arxiv.org/pdf/2206.05802)
- ğŸ“°[Specification gaming: the flip side of AI ingenuity](https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/)
- ğŸ“œ[Goal Misgeneralization in Deep Reinforcement Learning](https://arxiv.org/pdf/2105.14111)
- ğŸ“œ[Eliciting latent knowledge: How to tell if your eyes deceive you](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/)
- ğŸ“°[Discussion: Challenges with Unsupervised LLM Knowledge Discovery](https://www.lesswrong.com/posts/wtfvbsYjNHYYBmT3k/discussion-challenges-with-unsupervised-llm-knowledge-1)
- ğŸ“œ[Eliciting Latent Knowledge from â€œQuirkyâ€ Language Models](https://arxiv.org/pdf/2312.01037)

06/16/2024
- ğŸ“œ[Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B: A Technical Report](https://arxiv.org/pdf/2406.07394)
- ğŸ“œ[Improve Mathematical Reasoning in Language Models by Automated Process Supervision](https://arxiv.org/pdf/2406.06592)
- ğŸ“œ[Scalable MatMul-free Language Modeling](https://arxiv.org/pdf/2406.02528)

06/13/2024
- ğŸ“œ[Training Language Models with Language Feedback at Scale](https://arxiv.org/pdf/2303.16755)
- ğŸ“œ[Cooperative Inverse Reinforcement Learning](https://arxiv.org/pdf/1606.03137)
- ğŸ“œ[Representation Engineering: A Top-Down Approach To Ai Transparency](https://arxiv.org/pdf/2310.01405)
- ğŸ“œ[Improving Alignment and Robustness with Circuit Breakers](https://arxiv.org/pdf/2406.04313)

06/11/2024
- ğŸ“œ[Algorithms for Inverse Reinforcement Learning](https://people.eecs.berkeley.edu/~russell/papers/ml00-irl.pdf)
- ğŸ“œ[Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization](https://arxiv.org/pdf/1603.00448)
- ğŸ“œ[Deep Reinforcement Learning from Human Preferences](https://arxiv.org/pdf/1706.03741)
- ğŸ“œ[Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155)
- ğŸ“œ[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862)
- ğŸ“œ[Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/pdf/2212.08073)
- ğŸ“œ[RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback](https://arxiv.org/pdf/2309.00267)
- ğŸ“œ[Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences](https://arxiv.org/pdf/2404.03715)

06/10/2024
- ğŸ“–[Reinforcement Learning: An Introduction (Sutton & Barto), Chapters 1-3, 13](http://www.incompleteideas.net/book/RLbook2020.pdf)
- ğŸ“œ[Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347)

06/07/2024
- ğŸ“œ[Sigma-GPTs: A New Approach to Autoregressive Models](https://arxiv.org/pdf/2404.09562)
- ğŸ“°[Situational Awareness (Aschenbrenner)](https://situational-awareness.ai/)

06/06/2024
- ğŸ“œ[Adaptive Mixtures of Local Experts](https://people.engr.tamu.edu/rgutier/web_courses/cpsc636_s10/jacobs1991moe.pdf)
- ğŸ“œ[Hierarchical mixtures of experts and the EM algorithm](https://www.cs.toronto.edu/~hinton/absps/hme.pdf)
- ğŸ“œ[Outrageously Large Neural Networks: The Sparsely-Gated Mixture-Of-Experts Layer](https://arxiv.org/pdf/1701.06538)
- ğŸ“œ[Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961)
- ğŸ“œ[Unified Scaling Laws For Routed Language Models](https://arxiv.org/pdf/2202.01169)
- ğŸ“œ[Measuring the Effects of Data Parallelism on Neural Network Training](https://arxiv.org/pdf/1811.03600)
- ğŸ“œ[An Empirical Model of Large-Batch Training](https://arxiv.org/pdf/1812.06162)
- ğŸ“œ[The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/pdf/1803.03635)

06/05/2024
- ğŸ“œ[LLMs Canâ€™t Plan, But Can Help Planning in LLM-Modulo Frameworks](https://arxiv.org/pdf/2402.01817)
- ğŸŒ[EECS 498-007 Deep Learning for Computer Vision (Lectures 19-21)](https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/schedule.html)
- ğŸ“–[AI Safety Book (Hendrycks), Chapters 5.1-5.5](https://drive.google.com/file/d/1JN7-ZGx9KLqRJ94rOQVwRSa7FPZGl2OY/view)

06/04/2024
- ğŸ§ª[Found interpretable features in SAE](./projects/sae)

06/03/2024
- ğŸ§ª[Trained a SAE on GPT2-small](./projects/sae)

06/02/2024
- ğŸ“°[Apollo Research 1-year update](https://www.alignmentforum.org/posts/qK79p9xMxNaKLPuog/apollo-research-1-year-update)
- ğŸ“œ[Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning](https://arxiv.org/pdf/2405.12241)
- ğŸ“œ[The Local Interaction Basis: Identifying Computationally-Relevant and Sparsely Interacting Features in Neural Networks](https://arxiv.org/pdf/2405.10928)
- ğŸ“°[Sparsify: A mechanistic interpretability research agenda](https://www.alignmentforum.org/posts/64MizJXzyvrYpeKqm/sparsify-a-mechanistic-interpretability-research-agenda)
- ğŸ“œ[Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control](https://arxiv.org/pdf/2405.08366)

05/31/2024
- ğŸ“°[The Engineerâ€™s Interpretability Sequence](https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7)
- ğŸ“œ[Robustness May Be at Odds with Accuracy](https://arxiv.org/pdf/1805.12152)
- ğŸ“œ[Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/pdf/1905.02175)
- ğŸ§ª[ARENA 3.0, Chapter 1.4](https://arena3-chapter1-transformer-interp.streamlit.app/[1.4]_Superposition_&_SAEs)

05/30/2024
- ğŸ§ª[ARENA 3.0, Chapter 1.3](https://arena3-chapter1-transformer-interp.streamlit.app/[1.3]_Indirect_Object_Identification)

05/29/2024
- ğŸ§ª[ARENA 3.0, Chapter 1.2](https://arena3-chapter1-transformer-interp.streamlit.app/[1.2]_Intro_to_Mech_Interp)

05/28/2024
- ğŸ“œ[Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems](https://arxiv.org/pdf/2405.06624)
- ğŸ“œ[Suppressing Pink Elephants with Direct Principle Feedback](https://arxiv.org/pdf/2402.07896)

05/27/2024
- ğŸ“œ[Circuit Component Reuse Across Tasks In Transformer Language Models](https://arxiv.org/pdf/2310.08744)
- ğŸ“œ[How to use and interpret activation patching](https://arxiv.org/pdf/2404.15255)
- ğŸ“œ[Towards Automated Circuit Discovery for Mechanistic Interpretability](https://arxiv.org/pdf/2304.14997)
- ğŸ“°[Attribution Patching: Activation Patching At Industrial Scale](https://www.neelnanda.io/mechanistic-interpretability/attribution-patching)
- ğŸ“œ[Causal Scrubbing: a method for rigorously testing interpretability hypotheses](https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing)
- ğŸ“œ[Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks](https://arxiv.org/pdf/2207.13243)

05/24/2024
- ğŸ“œ[Grokking: Generalization Beyond Overfitting On Small Algorithmic Datasets](https://arxiv.org/pdf/2201.02177)
- ğŸ“°[A Mechanistic Interpretability Analysis of Grokking](https://www.alignmentforum.org/posts/N6WM6hs7RQMKDhYjB/a-mechanistic-interpretability-analysis-of-grokking)
- ğŸ“œ[Scaling Laws and Interpretability of Learning from Repeated Data](https://arxiv.org/pdf/2205.10487)
- ğŸ“œ[Deep Double Descent: Where Bigger Models And More Data Hurt](https://arxiv.org/pdf/1912.02292)
- ğŸ“œ[Interpretability In The Wild: A Circuit For Indirect Object Identification In Gpt-2 Small](https://arxiv.org/pdf/2211.00593)

05/23/2024
- ğŸ§ª[ARENA 3.0, Chapter 1.1](https://arena3-chapter1-transformer-interp.streamlit.app/[1.1]_Transformer_from_Scratch)

05/22/2024
- ğŸ“–[AI Safety Book (Hendrycks), Chapters 3.4-4.9](https://drive.google.com/file/d/1JN7-ZGx9KLqRJ94rOQVwRSa7FPZGl2OY/view)
- ğŸ“œ[Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)

05/20/2024
- ğŸ§ª[Finetuned GPT-2 models with custom dataset](./projects/bomgen/gpt2-finetune/)

05/17/2024
- ğŸ§ª[Created Resumable to support suspending/resuming training mid-epoch](./projects/resumable/resumable.py)

05/15/2024
- ğŸ§ª[My first neural style transfer](./projects/style-transfer/)
- ğŸŒ[EECS 498-007 Deep Learning for Computer Vision (Lectures 16-18)](https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/schedule.html)

05/14/2024
- ğŸ“°[Effort, a possibly new algorithm for LLM inference](https://kolinko.github.io/effort/index.html)
- ğŸ“°[Introduction to Weight Quantization](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c)
- ğŸ“œ[LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/pdf/2208.07339)
- ğŸ“œ[GPTQ (OPTQ): Accurate Post-training Quantization For Generative Pre-trained Transformers](https://arxiv.org/pdf/2210.17323)
- ğŸ“°[GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)
- ğŸ“œ[QuIP: 2-Bit Quantization of Large Language Models With Guarantees](https://arxiv.org/pdf/2307.13304)
- ğŸ“œ[QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/pdf/2402.04396)
- ğŸŒ[EECS 498-007 Deep Learning for Computer Vision (Lectures 14-15)](https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/schedule.html)
- ğŸ§ª[Set up repo & research log](./research-log.md)

05/10/2024
- ğŸ“œ[Receptance Weighted Key Value (RWKV)](https://arxiv.org/pdf/2305.13048)
- ğŸŒ[Ilya's 30u30 Deep Learning](https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE)

05/09/2024
- ğŸ“°[HuggingFace NLP Course](https://huggingface.co/learn/nlp-course)
- ğŸ“–[Alice's Adventures In a Differentiable Wonderland](https://arxiv.org/pdf/2404.17625)
- ğŸ“œ[An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale](https://arxiv.org/pdf/2010.11929)
- ğŸ“°[A Visual Guide to Vision Transformers ](https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html)

05/08/2024
- ğŸ“œ[xLSTM: Extended Long Short-Term Memory](https://arxiv.org/abs/2405.04517)

05/06/2024
- ğŸ“°[Inducing Unprompted Misalignment in LLMs](https://www.lesswrong.com/posts/ukTLGe5CQq9w8FMne/inducing-unprompted-misalignment-in-llms)
- ğŸ“°[Simple probes can catch sleeper agents](https://www.anthropic.com/research/probes-catch-sleeper-agents)
- ğŸ“œ[The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions](https://arxiv.org/abs/2404.13208)
- ğŸ“œ[Efficiently Modeling Long Sequences with Structured State Spaces (S4)](https://arxiv.org/pdf/2111.00396)
- ğŸ“œ[Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/pdf/2312.00752)
- ğŸ“œ[Transformer Circuits In-Context Learning & Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)
- ğŸ“œ[Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/)

05/05/2024
- ğŸ“°[Deep Reinforcement Learning Doesn't Work Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html)
- ğŸ“œ[Direct Preference Optimization: Your Language Model is Secretly a Reward Model (DPO)](https://arxiv.org/pdf/2305.18290)
- ğŸ“œ[Direct Preference Optimization with an Offset (ODPO)](https://arxiv.org/pdf/2402.10571)

05/03/2024
- ğŸ“œ[Improved Regularization of Convolutional Neural Networks with Cutout](https://arxiv.org/pdf/1708.04552)
- ğŸ“œ[AutoAugment: Learning Augmentation Strategies from Data](https://arxiv.org/pdf/1805.09501)
- ğŸ“œ[RandAugment: Practical automated data augmentation with a reduced search space](https://arxiv.org/pdf/1909.13719)
- ğŸ“œ[Augmix: A Simple Data Processing Method To Improve Robustness And Uncertainty](https://arxiv.org/pdf/1912.02781)
- ğŸ“œ[TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation](https://arxiv.org/pdf/2103.10158)
- ğŸ§ª[My first ResNet (LR search, schedulers, data augmentation)](./projects/cnns/imagenet/)

05/02/2024
- ğŸ“œ[Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour](https://arxiv.org/pdf/1706.02677)
- ğŸ§ª[My first ResNet (optimizer search)](./projects/cnns/imagenet/)

04/29/2024
- ğŸ“°[xFormers](https://github.com/facebookresearch/xformers)
- ğŸ“œ[SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions](https://arxiv.org/pdf/2403.16627)
- ğŸ“œ[Hyper-SD: Trajectory Segmented Consistency Model for Efficient Image Synthesis](https://arxiv.org/pdf/2404.13686)

04/25/2024
- ğŸ§ªExperiments with local models: oobabooga & a1111
- ğŸ“œ[OpenELM: An Efficient Language Model Family with Open Training and Inference Framework](https://arxiv.org/pdf/2404.14619)
- ğŸ“œ[Phi-3 Technical Report: A Highly Capable Language Modle Locally on Your Phone](https://arxiv.org/pdf/2404.14219)

04/24/2024
- ğŸ§ª[My first ResNet (training on CIFAR10)](./projects/cnns/imagenet/)

04/23/2024
- ğŸ§ª[My first ResNet (training on imagenette)](./projects/cnns/imagenet/)

04/22/2024
- ğŸ§ª[My first ResNet](./projects/cnns/imagenet/)

04/21/2024
- ğŸ§ª[My first CNN](./projects/cnns/lenet/)
- ğŸ“œ[Going Deeper with Convolutions](https://arxiv.org/pdf/1409.4842)
- ğŸ“œ[Deep Residual Learning for Image Recognition (ResNet)](https://arxiv.org/pdf/1512.03385.pdf)
- ğŸ“œ[Neural Architecture Search with Reinforcement Learning](https://arxiv.org/pdf/1611.01578)
- ğŸ“œ[ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](https://arxiv.org/pdf/1707.01083)
- ğŸ“œ[MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/pdf/1704.04861)
- ğŸ“œ[Gaussian Error Linear Units (GELUs)](https://arxiv.org/pdf/1606.08415)
- ğŸ“œ[GELU Activation Function in Deep Learning: A Comprehensive Mathematical Analysis and Performance](https://arxiv.org/pdf/2305.12073)
- ğŸŒ[EECS 498-007 Deep Learning for Computer Vision (Lectures 9-13)](https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/schedule.html)

04/20/2024
- ğŸŒ[EECS 498-007 Deep Learning for Computer Vision (Lectures 1-8)](https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/schedule.html)

04/19/2024
- ğŸ“œ[A Baseline For Detecting Misclassified And Out-of-distribution Examples In Neural Networks](https://arxiv.org/pdf/1610.02136)
- ğŸ“œ[Benchmarking Neural Network Robustness To Common Corruptions And Perturbations](https://arxiv.org/pdf/1903.12261)
- ğŸ“œ[Natural Adversarial Examples](https://arxiv.org/pdf/1907.07174)
- ğŸ“œ[The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization](https://arxiv.org/pdf/2006.16241)

04/17/2024
- ğŸ“œ[Testing Robustness Against Unforeseen Adversaries](https://arxiv.org/pdf/1908.08016)
- ğŸ“œ[HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refus](https://arxiv.org/pdf/2402.04249)
- ğŸ“œ[Aligning Ai With Shared Human Values](https://arxiv.org/pdf/2008.02275)
- ğŸ“–[AI Safety Book (Hendrycks), Chapters 1-3.3](https://drive.google.com/file/d/1JN7-ZGx9KLqRJ94rOQVwRSa7FPZGl2OY/view)
- ğŸ“°[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- ğŸ“œ[Training Compute-Optimal Large Language Models (Chinchilla)](https://arxiv.org/pdf/2203.15556.pdf)

04/15/2024
- ğŸ“œ[WaveNet: A Generative Model for Raw Audio (CNN)](https://arxiv.org/pdf/1609.03499)
- ğŸ“œ[Attention Is All You Need](https://arxiv.org/pdf/1706.0376)
- ğŸ“°[Yes You Should Understand Backprop (Karpathy)](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b)
- ğŸ“º[Let's build GPT: from scratch, in code, spelled out (Karpathy)](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- ğŸ“°[tiktoken](https://github.com/openai/tiktoken)
- ğŸ§ª[My first GPT](./projects/bomgen/my_gpt)

04/14/2024
- ğŸ“œ[Recurrent Neural Network Based Language Model](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)
- ğŸ“œ[Generating Sequences With Recurrent Neural Networks (LSTM)](https://arxiv.org/pdf/1308.0850)
- ğŸ“œ[On the Properties of Neural Machine Translation: Encoderâ€“Decoder Approaches (GRU)](https://arxiv.org/pdf/1409.1259)
- ğŸ§ª[My first RNN](./projects/rnn/rnn-manual.py)
- ğŸ§ª[My first GRU](./projects/rnn/gru.py)
- ğŸ“°[Gemma PyTorch](https://github.com/google/gemma_pytorch)

04/13/2024
- ğŸ“œ[SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot](https://arxiv.org/abs/2301.00774)

04/11/2024
- ğŸ§ª[My first FNN](./projects/fnn)

04/10/2024
- ğŸŒ[NN Zero to Hero (Karpathy)](https://github.com/karpathy/nn-zero-to-hero)
- ğŸ“œ[A Neural Probabilistic Language Model (MLP)](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
- ğŸ“œ[Adam: A Method For Stochastic Optimization](https://arxiv.org/pdf/1412.6980)
- ğŸ“œ[On Layer Normalization in the Transformer Architecture](https://arxiv.org/pdf/2002.04745)

04/09/2024
- ğŸŒ[makemore (Karpathy)](https://github.com/karpathy/makemore)

04/08/2024
- ğŸ“°[Faulty reward functions in the wild](https://openai.com/research/faulty-reward-functions)
- ğŸ“°[The Singular Value Decompositions of Transformer Weight Matrices are Highly Interpretable](https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight?ref=conjecture.dev)
- ğŸ“°[unRLHF - Efficiently Undoing LLM Safeguards](https://www.conjecture.dev/research/unrlhf-efficiently-undoing-llm-safeguards)
- ğŸ“œ[Interpreting Neural Networks through the Polytope Lens](https://arxiv.org/pdf/2211.12312)
- ğŸ“œ[Representational Strengths and Limitations of Transformers](https://arxiv.org/pdf/2306.02896)

04/02/2024
- ğŸŒ[micrograd (Karpathy)](https://github.com/karpathy/micrograd)
- ğŸ“œ[Layer Normalization](https://arxiv.org/pdf/1607.06450)

03/31/2024
- ğŸ“º[Implementing GPT-2 From Scratch (Nanda)](https://www.youtube.com/watch?v=dsjUDacBw8o&t=2611s)

03/28/2024
- ğŸŒ[Transformers - A Comprehensive Mechanistic Interpretability Explainer & Glossary (Nanda)](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=pndoEIqJ6GPvC1yENQkEfZYR&q=encode)
- ğŸ“º[What is a Transformer? (Nanda)](https://www.youtube.com/watch?v=bOYE6E8JrtU)
- ğŸ“œ[Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)
- ğŸ“œ[A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)